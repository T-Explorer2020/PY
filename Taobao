import json
from importlib import reload

import demjson
import urllib
import os
import time
import requests
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
import sys

reload(sys)
sys.setdefaultencoding('utf8')

path_name = u'保存的文件'  # 图片保存的文件夹名称
Myname = u'T恤宽松男款'  # 搜索关键字

path = os.getcwd()  
new_path = os.path.join(path, path_name)
if not os.path.isdir(new_path):
    os.mkdir(new_path)

def get_browser_text(url):
    browser = webdriver.Firefox(executable_path="C:\\Program Files (x86)\\Mozilla Firefox\\geckodriver.exe")
    try:
        browser.get(url)
        print(browser.page_source)
        browserdata = browser.page_source
        browser.close()
        # res = r'<a .*?>(.*?)</a>'
        res = r'<pre .*?>(.*?)</pre>'
        json_data = re.findall(res, browserdata, re.S | re.M)
        print
        json_data
        for value in json_data:
            print
            value

        dict_data = demjson.decode(json_data)
        print
        'dict_data:'
        print
        dict_data
        # print type(dict_data)
        return dict_data
    except:
        return ""

def getJSONText(url):
    try:
        page = urllib.urlopen(url)
        data = page.read()
        # print (data)
        # print (type(data))
        # dict_data = json.loads(data)
        dict_data = demjson.decode(data)
        # print dict_data
        # print type(dict_data)
        return dict_data
    except:
        return ""
        
def getHTMLText(url):
    try:
        data = urllib.urlopen(url).read()
        res = r'<dd class="tb-rate-(.*?)"'
        data_list = re.findall(res, data, re.S | re.M)
        print
        type(data_list)
        print
        data_list[0]
        # for value in mm:
        #   print value
        return data_list
    except:
        return ""
        
def getJSONText2(url):
    try:
        proxies = {
            "http": "http://221.10.159.234:1337",
            "https": "https://60.255.186.169:8888",
        }
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}
        data = requests.get(url, headers=headers, proxies=proxies).text
        print(data)
        print(type(data))
        # dict_data = json.loads(data)
        dict_data = demjson.decode(data)
        print
        dict_data
        print
        type(dict_data)
        return dict_data
    except:
        return ""

def getJSONText3(url):
    try:
        driver = webdriver.Chrome(
            executable_path="C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe")
        driver.get(url) 
        print(driver.find_element_by_xpath('/html/body').text)
        data = driver.find_element_by_xpath('/html/body').text
        print(data)
        print(type(data)
        dict_data = demjson.decode(data)
        print
        dict_data
        print
        type(dict_data)
        return dict_data
    except:
        return ""


def mytaobao_spider():
    for page in range(0, 60):

        url = (
                'http://pub.alimama.com/items/search.json?q=%s&_t=1531556288035&toPage=%d&perPageSize=50&auctionTag=&shopTag=yxjh&t=1531556427336&_tb_token_=7e73deef30e18&pvid=10_117.136.70.61_599_1531556285246' % (
            Myname, page))
        # url = ('http://pub.alimama.com/items/channel/qqhd.json?q=%s&channel=qqhd&_t=1531121449018&toPage=%d&perPageSize=50&shopTag=&t=1531125125414&_tb_token_=eeee6ee3be688&pvid=19_118.112.188.32_688_1531125125232'%(Myname,page))
        # url = ('http://pub.alimama.com/items/search.json?q=%E9%9E%8B%E5%AD%90&_t=1531368912715&auctionTag=&perPageSize=50&shopTag=yxjh&t=1531368913289&_tb_token_=e370663ebef17&pvid=10_118.112.188.32_9532_1531368912863')
        print
        url
        time.sleep(2)  
        url_data = getJSONText(url)
        for i in range(0, 50):
            time.sleep(1)
            try:
                # print type(url_data['data']['pageList'][i]['pictUrl'])
                pictUrl = url_data['data']['pageList'][i]['pictUrl'] 
                sellerId = url_data['data']['pageList'][i]['sellerId']  
                auctionUrl = url_data['data']['pageList'][i]['auctionUrl']  
                auctionId = url_data['data']['pageList'][i][
                    'auctionId'] 
                tkRate = url_data['data']['pageList'][i]['tkRate']  
                zkPrice = url_data['data']['pageList'][i]['zkPrice']  

                if tkRate > 10.00:
                    # time.sleep(1)
                    # print '详细信息:'
                    # print type(tkRate)
                    # print type(zkPrice)
                    # print '比率:%f' % (tkRate)
                    # print '价格:%f' % (zkPrice)
                    # print sellerId
                    # print auctionId
                    # print pictUrl
                    # print auctionUrl  
                    # print type(sellerId)
                    print
                    auctionUrl

                    sub_url = auctionUrl  
                    sub_url_data = getHTMLText(sub_url)  
                    print
                    type(sub_url_data)
                    print
                    len(sub_url_data)

                    if (len(sub_url_data) == 0):
                        info_url = ('https://world.taobao.com/item/%d.htm' % (auctionId))
                        info_data = urllib.urlopen(info_url).read()
                        res_info = r'<li class="([^s].*?)<img'
                        tmp_url_data = re.findall(res_info, info_data, re.S | re.M)
                        print
                        "tmp_url_data:"
                        for value1 in tmp_url_data:
                            print
                            value1

                        sub_url_data = []
                        score_list = [x[0:4] for x in tmp_url_data]  
                        print
                        'new_list:'
                        for score in score_list:
                            print
                            score
                            if score == 'down':
                                score = 'lower'  
                            sub_url_data.append(score)

                        print
                        '替换后的list元素:'
                        for level_data in sub_url_data:
                            print
                            level_data
                    
                    if ((not (sub_url_data[0] == 'lower')) and (not (sub_url_data[1] == 'lower')) and (
                            not (sub_url_data[2] == 'lower'))):
                        # for value in sub_url_data:
                        #   print value

                        mypictUrl = 'http:' + pictUrl 
                        picture_content = urllib.urlopen(mypictUrl).read()
                        picture_name = auctionUrl + '.jpg'  
                        print
                        picture_name
                        time.sleep(1)

                        
                        spider_info = 'url:' + url + '\n' + '  sub_url:' + sub_url + '\n' + '  淘宝链接:' + auctionUrl + '\n' + '  mypictUrl:' + mypictUrl + '\n\n'
                        try:
                            
                            index_num = picture_name.index('id=')
                            with open(path_name + '/' + picture_name[index_num:], 'wb') as code:
                                code.write(picture_content)
                            
                            with open(path_name + '/' + 'spider.txt', 'a') as spider_code:
                                spider_code.write(spider_info)
                        except (IOError, ZeroDivisionError) as e:
                            print
                            e
                            print
                            "Error: 没有找到图片文件或读取文件失败"
                        else:
                            print
                            "图片写入成功"

                        time.sleep(1)

            except (IndexError, KeyError, TypeError) as e:
                print
                e
                print
                "每件商品信息读取失败"
            else:
                pass
                print "每件商品的标签信息读取成功"


if __name__ == '__main__':
    mytaobao_spider()






